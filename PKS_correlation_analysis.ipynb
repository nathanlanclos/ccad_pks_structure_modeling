{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PKS Structural Feature Correlation Analysis\n",
        "\n",
        "This notebook provides a **comprehensive correlation analysis** between all structural features in the PKS dataset.\n",
        "\n",
        "## Goals\n",
        "1. Identify strongly correlated features (for dimensionality reduction)\n",
        "2. Discover unexpected relationships between structural properties\n",
        "3. Find features that best explain structural organization\n",
        "4. Generate hypotheses about structure-function relationships\n",
        "\n",
        "## Data Sources\n",
        "- Module macroproperties (size, surface area, volume, compactness)\n",
        "- Domain macroproperties\n",
        "- Inter-domain distances\n",
        "- Domain centroids and orientations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.cluster import hierarchy\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "pd.set_option('display.max_columns', 50)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "print(\"Libraries loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all data sources\n",
        "domain_mp = pd.read_csv('domain_macroproperties.csv', index_col=0)\n",
        "module_mp = pd.read_csv('MP_PKS.csv')\n",
        "combined_df = pd.read_csv('MP_IA_IDO_combined.csv', low_memory=False)\n",
        "ido_df = pd.read_csv('IDO_out.csv', low_memory=False)\n",
        "\n",
        "print(f\"Domain macroproperties: {domain_mp.shape}\")\n",
        "print(f\"Module macroproperties: {module_mp.shape}\")\n",
        "print(f\"Combined data: {combined_df.shape}\")\n",
        "print(f\"IDO data: {ido_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify numeric columns for correlation analysis\n",
        "def get_numeric_cols(df, exclude_patterns=['Unnamed', 'filename', 'zernike']):\n",
        "    \"\"\"Get numeric columns excluding specified patterns\"\"\"\n",
        "    numeric = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    return [c for c in numeric if not any(p.lower() in c.lower() for p in exclude_patterns)]\n",
        "\n",
        "module_numeric = get_numeric_cols(module_mp)\n",
        "combined_numeric = get_numeric_cols(combined_df)\n",
        "\n",
        "print(f\"Module numeric columns: {len(module_numeric)}\")\n",
        "print(f\"Combined numeric columns: {len(combined_numeric)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Module Macroproperties Correlations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Module macroproperties correlation matrix\n",
        "struct_cols = ['n_ca_atoms', 'n_heavy_atoms', 'radius_of_gyration_ca', 'sasa', 'ses_area', 'vdw_volume']\n",
        "available_struct = [c for c in struct_cols if c in module_mp.columns]\n",
        "\n",
        "corr_module = module_mp[available_struct].corr()\n",
        "\n",
        "# Heatmap\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "mask = np.triu(np.ones_like(corr_module, dtype=bool), k=1)\n",
        "sns.heatmap(corr_module, annot=True, cmap='RdBu_r', center=0, ax=ax, \n",
        "            fmt='.3f', mask=mask, square=True, linewidths=0.5)\n",
        "ax.set_title('Module Macroproperties Correlation Matrix', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print strong correlations\n",
        "print(\"\\nStrong correlations (|r| > 0.9):\")\n",
        "for i, col1 in enumerate(available_struct):\n",
        "    for j, col2 in enumerate(available_struct):\n",
        "        if i < j:\n",
        "            r = corr_module.loc[col1, col2]\n",
        "            if abs(r) > 0.9:\n",
        "                print(f\"  {col1} ↔ {col2}: r = {r:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scatter plot matrix for module properties\n",
        "fig = plt.figure(figsize=(14, 14))\n",
        "pd.plotting.scatter_matrix(module_mp[available_struct], \n",
        "                           figsize=(14, 14), \n",
        "                           diagonal='hist',\n",
        "                           alpha=0.3,\n",
        "                           s=10)\n",
        "plt.suptitle('Module Macroproperties: Pairwise Relationships', fontsize=14, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Inter-Domain Distance Correlations\n",
        "\n",
        "Analyze which domain-domain distances are correlated (suggesting structural constraints).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get distance columns\n",
        "distance_cols = [c for c in ido_df.columns if c.startswith('dist.')]\n",
        "print(f\"Total distance columns: {len(distance_cols)}\")\n",
        "\n",
        "# Focus on key catalytic domain distances (no linkers)\n",
        "def is_catalytic_distance(col):\n",
        "    \"\"\"Check if distance is between two catalytic (non-linker) domains\"\"\"\n",
        "    domains = col.replace('dist.', '').split('__')\n",
        "    return all(len(d) <= 3 or not d.endswith('L') for d in domains)\n",
        "\n",
        "catalytic_dist_cols = [c for c in distance_cols if is_catalytic_distance(c)]\n",
        "print(f\"Catalytic domain distances: {len(catalytic_dist_cols)}\")\n",
        "\n",
        "# Select key distances for analysis\n",
        "key_distances = ['dist.KS__AT', 'dist.KS__ACP', 'dist.AT__ACP', 'dist.KS__KR',\n",
        "                 'dist.KR__ACP', 'dist.DH__KR', 'dist.DH__ACP', 'dist.ER__KR',\n",
        "                 'dist.KS__DH', 'dist.AT__KR', 'dist.AT__DH']\n",
        "available_key = [d for d in key_distances if d in distance_cols]\n",
        "print(f\"\\nKey distances available: {len(available_key)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distance correlation matrix\n",
        "if len(available_key) >= 4:\n",
        "    dist_corr = ido_df[available_key].corr()\n",
        "    \n",
        "    # Clean labels\n",
        "    clean_labels = [c.replace('dist.', '').replace('__', '↔') for c in available_key]\n",
        "    dist_corr.index = clean_labels\n",
        "    dist_corr.columns = clean_labels\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "    sns.heatmap(dist_corr, annot=True, cmap='RdBu_r', center=0, ax=ax, \n",
        "                fmt='.2f', square=True, linewidths=0.5,\n",
        "                annot_kws={'size': 9})\n",
        "    ax.set_title('Inter-Domain Distance Correlations\\n(Higher correlation = Structurally coupled)', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Find strongest correlations\n",
        "    print(\"\\nStrongest distance correlations (|r| > 0.7):\")\n",
        "    for i, col1 in enumerate(available_key):\n",
        "        for j, col2 in enumerate(available_key):\n",
        "            if i < j:\n",
        "                r = ido_df[col1].corr(ido_df[col2])\n",
        "                if abs(r) > 0.7:\n",
        "                    label1 = col1.replace('dist.', '').replace('__', '↔')\n",
        "                    label2 = col2.replace('dist.', '').replace('__', '↔')\n",
        "                    print(f\"  {label1} ↔ {label2}: r = {r:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Cross-Feature Correlations\n",
        "\n",
        "Correlate macroproperties with inter-domain distances.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-correlate macro properties with distances\n",
        "# Use combined_df which has both\n",
        "macro_cols = [c for c in available_struct if c in combined_df.columns]\n",
        "dist_cols_combined = [c for c in available_key if c in combined_df.columns]\n",
        "\n",
        "if macro_cols and dist_cols_combined:\n",
        "    # Calculate cross-correlation matrix\n",
        "    cross_corr = pd.DataFrame(index=macro_cols, columns=dist_cols_combined)\n",
        "    \n",
        "    for mc in macro_cols:\n",
        "        for dc in dist_cols_combined:\n",
        "            r = combined_df[mc].corr(combined_df[dc])\n",
        "            cross_corr.loc[mc, dc] = r\n",
        "    \n",
        "    cross_corr = cross_corr.astype(float)\n",
        "    \n",
        "    # Heatmap\n",
        "    fig, ax = plt.subplots(figsize=(14, 6))\n",
        "    clean_dist_labels = [c.replace('dist.', '').replace('__', '↔') for c in dist_cols_combined]\n",
        "    cross_corr.columns = clean_dist_labels\n",
        "    \n",
        "    sns.heatmap(cross_corr, annot=True, cmap='RdBu_r', center=0, ax=ax,\n",
        "                fmt='.2f', linewidths=0.5)\n",
        "    ax.set_title('Macroproperties vs Inter-Domain Distances\\n(Correlation Matrix)', fontsize=14)\n",
        "    ax.set_xlabel('Inter-Domain Distance')\n",
        "    ax.set_ylabel('Macroproperty')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Find strongest cross-correlations\n",
        "    print(\"\\nStrongest cross-correlations (|r| > 0.3):\")\n",
        "    for mc in macro_cols:\n",
        "        for dc in dist_cols_combined:\n",
        "            r = float(cross_corr.loc[mc, dc.replace('dist.', '').replace('__', '↔') if dc.replace('dist.', '').replace('__', '↔') in cross_corr.columns else dc])\n",
        "            if abs(r) > 0.3:\n",
        "                dc_label = dc.replace('dist.', '').replace('__', '↔')\n",
        "                print(f\"  {mc} vs {dc_label}: r = {r:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Hierarchical Clustering of Features\n",
        "\n",
        "Cluster features by correlation to identify groups of related measurements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select top features for clustering\n",
        "# Combine macroproperties and key distances\n",
        "all_features = macro_cols + dist_cols_combined\n",
        "available_for_cluster = [c for c in all_features if c in combined_df.columns]\n",
        "\n",
        "# Drop rows with too many NaN values\n",
        "cluster_data = combined_df[available_for_cluster].dropna(thresh=len(available_for_cluster)*0.5)\n",
        "cluster_data = cluster_data.dropna(axis=1, thresh=len(cluster_data)*0.5)\n",
        "\n",
        "print(f\"Features for clustering: {cluster_data.shape[1]}\")\n",
        "print(f\"Samples: {cluster_data.shape[0]}\")\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = cluster_data.corr()\n",
        "\n",
        "# Hierarchical clustering dendrogram\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "# Dendrogram\n",
        "ax = axes[0]\n",
        "from scipy.spatial.distance import squareform\n",
        "\n",
        "# Convert correlation to distance (1 - |corr|)\n",
        "dist_matrix = 1 - np.abs(corr_matrix)\n",
        "np.fill_diagonal(dist_matrix.values, 0)\n",
        "\n",
        "# Linkage\n",
        "condensed = squareform(dist_matrix)\n",
        "linkage = hierarchy.linkage(condensed, method='average')\n",
        "\n",
        "# Plot dendrogram\n",
        "clean_labels = [c.replace('dist.', '').replace('__', '↔') for c in corr_matrix.columns]\n",
        "dendro = hierarchy.dendrogram(linkage, labels=clean_labels, ax=ax, leaf_rotation=90)\n",
        "ax.set_ylabel('Distance (1 - |correlation|)')\n",
        "ax.set_title('Feature Clustering by Correlation')\n",
        "\n",
        "# Clustered heatmap\n",
        "ax = axes[1]\n",
        "order = dendro['leaves']\n",
        "ordered_corr = corr_matrix.iloc[order, order]\n",
        "ordered_corr.index = [clean_labels[i] for i in order]\n",
        "ordered_corr.columns = [clean_labels[i] for i in order]\n",
        "\n",
        "sns.heatmap(ordered_corr, cmap='RdBu_r', center=0, ax=ax, square=True,\n",
        "            xticklabels=True, yticklabels=True)\n",
        "ax.set_title('Reordered Correlation Matrix')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Domain Centroid Coordinate Correlations\n",
        "\n",
        "Analyze relationships between domain spatial positions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Centroid coordinate correlations\n",
        "centroid_cols = [c for c in ido_df.columns if c.startswith('centroid.')]\n",
        "print(f\"Centroid columns: {len(centroid_cols)}\")\n",
        "\n",
        "# Extract key domain centroids (X, Y, Z for main catalytic domains)\n",
        "key_domains = ['KS', 'AT', 'ACP', 'KR', 'DH', 'ER']\n",
        "centroid_key = []\n",
        "for dom in key_domains:\n",
        "    for axis in ['X', 'Y', 'Z']:\n",
        "        col = f'centroid.{dom}.{axis}'\n",
        "        if col in centroid_cols:\n",
        "            centroid_key.append(col)\n",
        "\n",
        "print(f\"Key centroid coordinates: {len(centroid_key)}\")\n",
        "\n",
        "if len(centroid_key) >= 9:\n",
        "    # Correlation of X coordinates (how domains line up on X axis)\n",
        "    x_cols = [c for c in centroid_key if c.endswith('.X')]\n",
        "    if len(x_cols) >= 3:\n",
        "        x_corr = ido_df[x_cols].corr()\n",
        "        \n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "        \n",
        "        for i, axis in enumerate(['X', 'Y', 'Z']):\n",
        "            ax = axes[i]\n",
        "            axis_cols = [c for c in centroid_key if c.endswith(f'.{axis}')]\n",
        "            axis_corr = ido_df[axis_cols].corr()\n",
        "            \n",
        "            # Clean labels\n",
        "            clean = [c.replace('centroid.', '').replace(f'.{axis}', '') for c in axis_cols]\n",
        "            axis_corr.index = clean\n",
        "            axis_corr.columns = clean\n",
        "            \n",
        "            sns.heatmap(axis_corr, annot=True, cmap='RdBu_r', center=0, ax=ax,\n",
        "                       fmt='.2f', square=True)\n",
        "            ax.set_title(f'{axis}-Coordinate Correlations')\n",
        "        \n",
        "        plt.suptitle('Domain Centroid Coordinate Correlations', fontsize=14, y=1.02)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Rotation Matrix Analysis\n",
        "\n",
        "Analyze correlations in domain orientations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rotation matrix elements correlation\n",
        "rotation_cols = [c for c in ido_df.columns if c.startswith('R.')]\n",
        "print(f\"Rotation matrix columns: {len(rotation_cols)}\")\n",
        "\n",
        "if len(rotation_cols) == 9:\n",
        "    # R.00, R.01, R.02, R.10, R.11, R.12, R.20, R.21, R.22\n",
        "    rot_corr = ido_df[rotation_cols].corr()\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    sns.heatmap(rot_corr, annot=True, cmap='RdBu_r', center=0, ax=ax, \n",
        "               fmt='.2f', square=True)\n",
        "    ax.set_title('Rotation Matrix Element Correlations\\n(Reference → Module orientation)', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Analyze diagonal vs off-diagonal (diagonal = axis preservation)\n",
        "    diag = ['R.00', 'R.11', 'R.22']\n",
        "    off_diag = ['R.01', 'R.02', 'R.10', 'R.12', 'R.20', 'R.21']\n",
        "    \n",
        "    print(\"\\nRotation Matrix Statistics:\")\n",
        "    print(\"-\"*50)\n",
        "    print(f\"Diagonal elements (axis preservation):\")\n",
        "    for col in diag:\n",
        "        if col in ido_df.columns:\n",
        "            vals = ido_df[col].dropna()\n",
        "            print(f\"  {col}: mean={vals.mean():.3f}, std={vals.std():.3f}\")\n",
        "    \n",
        "    print(f\"\\nOff-diagonal elements (axis mixing):\")\n",
        "    for col in off_diag:\n",
        "        if col in ido_df.columns:\n",
        "            vals = ido_df[col].dropna()\n",
        "            print(f\"  {col}: mean={vals.mean():.3f}, std={vals.std():.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Full Correlation Network\n",
        "\n",
        "Identify the most connected features in the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find features with most strong correlations\n",
        "threshold = 0.6\n",
        "\n",
        "# Build adjacency from correlation\n",
        "all_numeric = combined_df.select_dtypes(include=[np.number])\n",
        "# Remove columns with too many NaN\n",
        "valid_cols = all_numeric.columns[all_numeric.isnull().mean() < 0.5]\n",
        "all_numeric = all_numeric[valid_cols]\n",
        "\n",
        "# Sample for large correlation matrix\n",
        "if len(all_numeric.columns) > 100:\n",
        "    # Select mix of macro, distance, and centroid columns\n",
        "    selected_cols = []\n",
        "    for prefix in ['n_', 'radius', 'sasa', 'ses_', 'vdw_', 'dist.KS', 'dist.AT', 'dist.ACP', \n",
        "                   'centroid.KS', 'centroid.AT', 'centroid.ACP']:\n",
        "        matching = [c for c in all_numeric.columns if c.startswith(prefix)][:3]\n",
        "        selected_cols.extend(matching)\n",
        "    selected_cols = list(set(selected_cols))[:40]\n",
        "    analysis_df = all_numeric[selected_cols]\n",
        "else:\n",
        "    analysis_df = all_numeric\n",
        "\n",
        "print(f\"Analyzing {len(analysis_df.columns)} features\")\n",
        "\n",
        "# Correlation matrix\n",
        "full_corr = analysis_df.corr()\n",
        "\n",
        "# Count strong correlations per feature\n",
        "strong_corr_counts = {}\n",
        "for col in full_corr.columns:\n",
        "    count = (abs(full_corr[col]) > threshold).sum() - 1  # exclude self\n",
        "    strong_corr_counts[col] = count\n",
        "\n",
        "# Top connected features\n",
        "top_connected = sorted(strong_corr_counts.items(), key=lambda x: x[1], reverse=True)[:15]\n",
        "\n",
        "print(f\"\\nMost Connected Features (|r| > {threshold}):\")\n",
        "print(\"-\"*60)\n",
        "for feat, count in top_connected:\n",
        "    clean_feat = feat.replace('dist.', '').replace('__', '↔').replace('centroid.', 'c.')\n",
        "    print(f\"  {clean_feat:40s}: {count:3d} strong correlations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. Summary and Key Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "print(\"=\"*70)\n",
        "print(\"CORRELATION ANALYSIS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. HIGHLY CORRELATED FEATURE PAIRS (r > 0.9):\")\n",
        "print(\"-\"*50)\n",
        "high_corr_pairs = []\n",
        "for i, col1 in enumerate(analysis_df.columns):\n",
        "    for j, col2 in enumerate(analysis_df.columns):\n",
        "        if i < j:\n",
        "            r = analysis_df[col1].corr(analysis_df[col2])\n",
        "            if abs(r) > 0.9:\n",
        "                high_corr_pairs.append((col1, col2, r))\n",
        "\n",
        "high_corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
        "for col1, col2, r in high_corr_pairs[:10]:\n",
        "    clean1 = col1.replace('dist.', '').replace('__', '↔')[:25]\n",
        "    clean2 = col2.replace('dist.', '').replace('__', '↔')[:25]\n",
        "    print(f\"  {clean1:25s} ↔ {clean2:25s}: r={r:.3f}\")\n",
        "\n",
        "print(f\"\\n  Total pairs with |r| > 0.9: {len(high_corr_pairs)}\")\n",
        "\n",
        "print(\"\\n2. REDUNDANT FEATURES (Consider removing for ML):\")\n",
        "print(\"-\"*50)\n",
        "if len(high_corr_pairs) > 0:\n",
        "    redundant = set()\n",
        "    for col1, col2, r in high_corr_pairs:\n",
        "        if col2 not in redundant:  # Keep first, mark second as redundant\n",
        "            redundant.add(col2)\n",
        "    print(f\"  {len(redundant)} features are highly correlated with others\")\n",
        "    for f in list(redundant)[:10]:\n",
        "        print(f\"    - {f[:50]}\")\n",
        "\n",
        "print(\"\\n3. INTERPRETATIONS:\")\n",
        "print(\"-\"*50)\n",
        "print(\"  • Size metrics (n_atoms, volume, SASA) are highly correlated\")\n",
        "print(\"  • Adjacent domain distances show correlation (structural constraints)\")\n",
        "print(\"  • Centroid coordinates on same axis indicate linear arrangement\")\n",
        "print(\"  • Rotation matrix structure reveals preferred orientations\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
